{"config":{"lang":["en"],"prebuild_index":false,"separator":"[\\s\\-]+"},"docs":[{"location":"","text":"WEB MINNING Nama : Aldi Saputra Nim : 180411100087 Dosen : Mulaab Mata Kuliah : Web Minning A Jurusan : Teknik Informatika 2018 Materi Crawling Data Preprocessing Modelling Evaluasi Crawling Data Web crawling adalah proses di mana search engine menemukan konten yang di-update di sebuah situs atau halaman baru, perubahan situs, atau link yang mati. Contoh web crawling Beberapa web crawler lain selain Googlebot adalah sebagai berikut: Bingbot dari Bing Slurp Bot dari Yahoo DuckDuckBot dari DuckDuckGO Baiduspider dari Baidu (mesin pencari dari China) Installation pip install tweepy Ngecrawl Data import tweepy import csv Buat variable untuk menampung data access_token=\"\" access_token_secret=\"\" consumer_key=\"\" consumer_key_secret=\"\" API tweepy auth = tweepy.OAuthHandler(consumer_key,consumer_key_secret) api = tweepy.API(auth) Membuka/Membuat sebuah file untuk memasukkan data ke csv csvFile = open('nama-file.csv', 'w', encoding='utf-8') Menggunakan csv read csvWriter = csv.writer(csvFile) Melakukan crawling data for tweet in tweepy.Cursor(api.search, q='#Python -filter:retweets', tweet_mode='extended',lang=\"id\", since='2021-01-01', until='2021-01-10').items(100): text = tweet.full_text user = tweet.user.name created = tweet.created_at csvWriter.writerow([created, text.encode('utf-8'), user]) csvWriter = csv.writer(csvFile) csvFile.close() Preprocessing Pengertian Preprocessing merupakan salah satu tahapan yang penting untuk data pada proses mining. Data yang digunakan dalam proses mining tidak selamanya dalam kondisi yang ideal untuk diproses. Tahap Text Preprocessing adalah tahapan dimana aplikasi melakukan seleksi data yang akan diproses pada setiap dokumen. Proses preprocessing ini meliputi (1) case folding , (2) tokenisasi, (3) filtering , dan (4) stemming**. 1) Case Folding peran Case Folding dibutuhkan dalam mengkonversi keseluruhan teks dalam dokumen menjadi suatu bentuk standar (biasanya huruf kecil atau lowercase) 2) Tokenisasi Tahap tokenisasi adalah tahap pemotongan string input berdasarkan tiap kata yang menyusunnya. Contoh dari tahap ini dapat dilihat pada gambar dibawah ini. 3) Filtering Tahap Filtering adalah tahap mengambil kata-kata penting dari hasil token.Stoplist/stopword adalah kata-kata yang tidak deskriptif yang dapat dibuang dalam pendekatan bag-of-words. Contoh stopwords adalah \u201cyang\u201d , \u201cdan\u201d , \u201cdi\u201d , \u201cdari\u201d dan seterusnya. 4) Stemming Teknik Stemming diperlukan selain untuk memperkecil jumlah indeks yang berbeda dari suatu dokumen, juga untuk melakukan pengelompokan kata-kata lain yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapatkan imbuhan yang berbeda. Contoh Program Preprocessing buat file function.py seperti berikut import re #regular expression import xlwt #library untuk membaca data pada excel from nltk.tokenize import word_tokenize #libray untuk tokenizing #Sastrawi Library untuk Stemming dan Stopword Removal Data Set Bahasa Indonesia from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory factori = StemmerFactory() stemmer = factori.create_stemmer() factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() class hadisClass(object): def __init__(self,hadis,k1,k2,k3): self.hadis = hadis self.k1 = k1 self.k2 = k2 self.k3 = k3 def openFile(wb): hadisContent = [] items= [] for sheet in wb.sheets(): num_row, num_col = sheet.nrows,4 for row in range(num_row): values = [] for col in range(num_col): if col == 0: x = (sheet.cell(row,col).value) value = (sheet.cell(row,col).value) values.append(value) hadisContent.append(x) item = hadisClass(*values) items.append(item) return hadisContent, items def prepro(datahadis): dhadis=[] for i in datahadis: cleanning = re.sub('[^a-zA-z\\s]','', i) casefold = cleanning.lower(); stopW = stopword.remove(casefold) stemming = stemmer.stem(stopW) tokens = word_tokenize(stemming) dhadis.append(tokens) return dhadis Dalam folder sama , buatlah file baru , coba.py seperti berikut import function as fc from xlrd import open_workbook wb = open_workbook('book1.xlsx') data, items = fc.openFile(wb) datatest = [] for j in data : datatest.append(j) datatest = fc.prepro(datatest) print(datatest) Ini adalah Hasilnya Modelling pengertian Yaitu subdivisi dari interkaso manusia dan komputer yang menggambarkan proses membangun dan memodifikasi pemahaman konseptual pengguna dimana tujuan dari pemodelan ini adalah penyesuaian dan adaptasi sistem dengan kebutuhan spesifik dari pengguna Text clustering Text Clustering adalah salah satu operasi pada text mining untuk mengelompokkan dokumen yang memiliki kesamaan isi. Pengelompokan data berdasarkan informasi yang diperoleh dari data yang menjelaskan hubunganantar objek dengan prinsip untuk memaksimalkan kesamaan antar anggota satukelas dan meminimumkan kesamaan antar kelas/cluster. Classification Classification, atau \u2018supervised induction\u2019, barangkali adalah tugas dalam data mining yang paling umum. Tujuan \u2018classification\u2019 adalah untuk menganalisa data historis yang disimpan dalam database dan secara otomatis menghasilkan suatu model yang bisa memprediksi perilaku di masa mendatang. Model induksi ini terdiri dari generalisasi pada baris-baris data yang digunakan untuk pelatihan, yang akan membantu membedakan class-class standar. klasifikasi data dengan decision tree Ikuti langkah berikut import pandas from pandas.plotting import scatter_matrix import matplotlib.pyplot as plt get_ipython().run_line_magic('matplotlib', 'inline') import numpy as np from sklearn.tree import DecisionTreeClassifier Memanggil data yang akan di proses url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species'] iris = pandas.read_csv(url, names=names) Memisahkan data yang merupakan fitur dengan data yang merupakan label kelas. Untuk memisahkan data tersebut, maka dapat digunakan *method \u201cdrop\u201d . Jika method drop()* digunakan untuk menghapus kolom, maka sebutkan nama kolom yang ingin dihapus dan set nilai Axis=1 . Sedangkan jika baris yang ingin dihapus maka tetapkan nilai Axis=0 . Untuk data yang merupakan fitur disimpan pada variabel X , sedangkan data yang merupakan label kelas disimpan pada variabel y . X = iris.drop('species', axis=1) y = iris['species'] Membagi data menjadi data latih ( training dat a) dan data uji ( test data ). Sebelumnya perlu dilakukan import library dari sklearn. from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) Membuat model klasifikasi dari data latih dan data uji yang telah ditetapkan menggunakan decision tree. classifier = DecisionTreeClassifier() classifier.fit(X_train, y_train) Menyimpan hasil prediksi yang diperoleh y_pred = classifier.predict(X_test) Menghitung akurasi dari hasil prediksi klasifikasi dengan menggunakan confussion matrix. from sklearn.metrics import classification_report, confusion_matrix print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) Menampilkan hasil prediksi df=pandas.DataFrame({'Actual':y_test, 'Predicted':y_pred}) df Untuk menampilkan model tree maka perlu instalasi graphvis terlebih dahulu melalui terminal. Kembali pada halaman depan jupyter notebook, pilih new, kemudian pilih terminal sehingga tampilan terminal akan muncul di tab baru. Pada terminal, ketikkan kode di bawah ini untuk menginstal graphpvis dari cloud anaconda. conda install -c conda-forge python-graphviz conda install -c conda-forge/label/broken python-graphviz conda install -c conda-forge/label/cf201901 python-graphviz Menyiapkan feature_names dan class_names dimana merupakan variabel yang perlu disiapkan untuk menggunakan graphvis . X.columns digunakan sebagai array yang menyimpan nama fitur ( feature_names ), sedangkan y.columns digunakan sebagai array untuk menyimpan nama kelas ( class_names ). X.columns = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width'] y.columns = ['Iris-setosa', 'Iris-virginica', 'Iris-versicolor'] Melakukan import library graphvis terlebih dulu kemudian menyimpannya sebagai dot file. from sklearn.tree import export_graphviz # Export as dot file export_graphviz(classifier, out_file='tree.dot', feature_names = X.columns, class_names = y.columns, rounded = True, proportion = False, precision = 2, filled = True) # Convert to png using system command (requires Graphviz) from subprocess import call call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600']) # Display in jupyter notebook from IPython.display import Image Image(filename = 'tree.png') Evaluasi Metode evaluasi Ringkasan otomatis Ada beberapa ukuran untuk mengevaluasi ringkasan otomatis semenjak awal 2000. ROUGE adalah yang paling banyak digunakan untuk mengevaluasi secara otomatis. Berikut metode yang paling banyak digunakan ROUGE-n ROUGE-L ROUGE-W ROUGE-S ROUGE-SU Implementasi Rouge dengan python Install pip install easy-rouge from rouge.rouge import rouge_n_sentence_level summary_sentence = 'the capital of China is Beijing'.split() reference_sentence = 'Beijing is the capital of China'.split() # Calculate ROUGE-2. recall, precision, rouge = rouge_n_sentence_level(summary_sentence, reference_sentence, 2) print('ROUGE-2-R', recall) print('ROUGE-2-P', precision) print('ROUGE-2-F', rouge) ROUGE-2-R 0.6 ROUGE-2-P 0.6 ROUGE-2-F 0.6 ROUGE-2-R 0.6 Implementasi ke-2 from rouge import rouge_n_sentence_level from rouge import rouge_l_sentence_level from rouge import rouge_n_summary_level from rouge import rouge_l_summary_level from rouge import rouge_w_sentence_level from rouge import rouge_w_summary_level reference_sentence = 'the police killed the gunman'.split() summary_sentence = 'the gunman police killed'.split() print('Sentence level:') score = rouge_n_sentence_level(summary_sentence, reference_sentence, 1) print('ROUGE-1: %f' % score.f1_measure) _, _, rouge_2 = rouge_n_sentence_level(summary_sentence, reference_sentence, 2) print('ROUGE-2: %f' % rouge_2) _, _, rouge_l = rouge_l_sentence_level(summary_sentence, reference_sentence) print('ROUGE-L: %f' % rouge_l) _, _, rouge_w = rouge_w_sentence_level(summary_sentence, reference_sentence) print('ROUGE-W: %f' % rouge_w) Sentence level: ROUGE-1: 0.888889 ROUGE-2: 0.571429 ROUGE-L: 0.666667 ROUGE-W: 0.550527 from rouge import rouge_n_sentence_level from rouge import rouge_l_sentence_level from rouge import rouge_n_summary_level from rouge import rouge_l_summary_level from rouge import rouge_w_sentence_level from rouge import rouge_w_summary_level reference_sentences = ['The gunman was shot dead by the police before more people got hurt'.split(), 'This tragedy causes lives of five , the gunman included'.split(), 'The motivation of the gunman remains unclear'.split(), ] summary_sentences = [ 'Police killed the gunman . no more people got hurt'.split(), 'Five people got killed including the gunman'.split(), 'It is unclear why the gunman killed people'.split(), ] print('Summary level:') _, _, rouge_1 = rouge_n_summary_level(summary_sentences, reference_sentences, 1) print('ROUGE-1: %f' % rouge_1) _, _, rouge_2 = rouge_n_summary_level(summary_sentences, reference_sentences, 2) print('ROUGE-2: %f' % rouge_2) _, _, rouge_l = rouge_l_summary_level(summary_sentences, reference_sentences) print('ROUGE-L: %f' % rouge_l) _, _, rouge_w = rouge_w_summary_level(summary_sentences, reference_sentences) print('ROUGE-W: %f' % rouge_w) Hasilnya Summary level: ROUGE-1: 0.400000 ROUGE-2: 0.188679 ROUGE-L: 0.327273 ROUGE-W: 0.200430 recall, precision, rouge_1 = rouge_n_summary_level(summary_sentences, reference_sentences, 1) print('ROUGE-2-R', recall) print('ROUGE-2-P', precision) print('ROUGE-2-F', rouge) Hasilnya ROUGE-2-R 0.36666666666666664 ROUGE-2-P 0.44 ROUGE-2-F 0.6","title":"WEB MINNING"},{"location":"#web-minning","text":"Nama : Aldi Saputra Nim : 180411100087 Dosen : Mulaab Mata Kuliah : Web Minning A Jurusan : Teknik Informatika 2018","title":"WEB MINNING"},{"location":"#materi","text":"Crawling Data Preprocessing Modelling Evaluasi","title":"Materi"},{"location":"#crawling-data","text":"Web crawling adalah proses di mana search engine menemukan konten yang di-update di sebuah situs atau halaman baru, perubahan situs, atau link yang mati.","title":"Crawling Data"},{"location":"#contoh-web-crawling","text":"Beberapa web crawler lain selain Googlebot adalah sebagai berikut: Bingbot dari Bing Slurp Bot dari Yahoo DuckDuckBot dari DuckDuckGO Baiduspider dari Baidu (mesin pencari dari China) Installation pip install tweepy Ngecrawl Data import tweepy import csv Buat variable untuk menampung data access_token=\"\" access_token_secret=\"\" consumer_key=\"\" consumer_key_secret=\"\" API tweepy auth = tweepy.OAuthHandler(consumer_key,consumer_key_secret) api = tweepy.API(auth) Membuka/Membuat sebuah file untuk memasukkan data ke csv csvFile = open('nama-file.csv', 'w', encoding='utf-8') Menggunakan csv read csvWriter = csv.writer(csvFile) Melakukan crawling data for tweet in tweepy.Cursor(api.search, q='#Python -filter:retweets', tweet_mode='extended',lang=\"id\", since='2021-01-01', until='2021-01-10').items(100): text = tweet.full_text user = tweet.user.name created = tweet.created_at csvWriter.writerow([created, text.encode('utf-8'), user]) csvWriter = csv.writer(csvFile) csvFile.close()","title":"Contoh web crawling"},{"location":"#preprocessing","text":"","title":"Preprocessing"},{"location":"#pengertian","text":"Preprocessing merupakan salah satu tahapan yang penting untuk data pada proses mining. Data yang digunakan dalam proses mining tidak selamanya dalam kondisi yang ideal untuk diproses. Tahap Text Preprocessing adalah tahapan dimana aplikasi melakukan seleksi data yang akan diproses pada setiap dokumen. Proses preprocessing ini meliputi (1) case folding , (2) tokenisasi, (3) filtering , dan (4) stemming**. 1) Case Folding peran Case Folding dibutuhkan dalam mengkonversi keseluruhan teks dalam dokumen menjadi suatu bentuk standar (biasanya huruf kecil atau lowercase) 2) Tokenisasi Tahap tokenisasi adalah tahap pemotongan string input berdasarkan tiap kata yang menyusunnya. Contoh dari tahap ini dapat dilihat pada gambar dibawah ini. 3) Filtering Tahap Filtering adalah tahap mengambil kata-kata penting dari hasil token.Stoplist/stopword adalah kata-kata yang tidak deskriptif yang dapat dibuang dalam pendekatan bag-of-words. Contoh stopwords adalah \u201cyang\u201d , \u201cdan\u201d , \u201cdi\u201d , \u201cdari\u201d dan seterusnya. 4) Stemming Teknik Stemming diperlukan selain untuk memperkecil jumlah indeks yang berbeda dari suatu dokumen, juga untuk melakukan pengelompokan kata-kata lain yang memiliki kata dasar dan arti yang serupa namun memiliki bentuk atau form yang berbeda karena mendapatkan imbuhan yang berbeda.","title":"Pengertian"},{"location":"#contoh-program-preprocessing","text":"buat file function.py seperti berikut import re #regular expression import xlwt #library untuk membaca data pada excel from nltk.tokenize import word_tokenize #libray untuk tokenizing #Sastrawi Library untuk Stemming dan Stopword Removal Data Set Bahasa Indonesia from Sastrawi.Stemmer.StemmerFactory import StemmerFactory from Sastrawi.StopWordRemover.StopWordRemoverFactory import StopWordRemoverFactory factori = StemmerFactory() stemmer = factori.create_stemmer() factory = StopWordRemoverFactory() stopword = factory.create_stop_word_remover() class hadisClass(object): def __init__(self,hadis,k1,k2,k3): self.hadis = hadis self.k1 = k1 self.k2 = k2 self.k3 = k3 def openFile(wb): hadisContent = [] items= [] for sheet in wb.sheets(): num_row, num_col = sheet.nrows,4 for row in range(num_row): values = [] for col in range(num_col): if col == 0: x = (sheet.cell(row,col).value) value = (sheet.cell(row,col).value) values.append(value) hadisContent.append(x) item = hadisClass(*values) items.append(item) return hadisContent, items def prepro(datahadis): dhadis=[] for i in datahadis: cleanning = re.sub('[^a-zA-z\\s]','', i) casefold = cleanning.lower(); stopW = stopword.remove(casefold) stemming = stemmer.stem(stopW) tokens = word_tokenize(stemming) dhadis.append(tokens) return dhadis Dalam folder sama , buatlah file baru , coba.py seperti berikut import function as fc from xlrd import open_workbook wb = open_workbook('book1.xlsx') data, items = fc.openFile(wb) datatest = [] for j in data : datatest.append(j) datatest = fc.prepro(datatest) print(datatest) Ini adalah Hasilnya","title":"Contoh Program Preprocessing"},{"location":"#modelling","text":"","title":"Modelling"},{"location":"#pengertian_1","text":"Yaitu subdivisi dari interkaso manusia dan komputer yang menggambarkan proses membangun dan memodifikasi pemahaman konseptual pengguna dimana tujuan dari pemodelan ini adalah penyesuaian dan adaptasi sistem dengan kebutuhan spesifik dari pengguna","title":"pengertian"},{"location":"#text-clustering","text":"Text Clustering adalah salah satu operasi pada text mining untuk mengelompokkan dokumen yang memiliki kesamaan isi. Pengelompokan data berdasarkan informasi yang diperoleh dari data yang menjelaskan hubunganantar objek dengan prinsip untuk memaksimalkan kesamaan antar anggota satukelas dan meminimumkan kesamaan antar kelas/cluster.","title":"Text clustering"},{"location":"#classification","text":"Classification, atau \u2018supervised induction\u2019, barangkali adalah tugas dalam data mining yang paling umum. Tujuan \u2018classification\u2019 adalah untuk menganalisa data historis yang disimpan dalam database dan secara otomatis menghasilkan suatu model yang bisa memprediksi perilaku di masa mendatang. Model induksi ini terdiri dari generalisasi pada baris-baris data yang digunakan untuk pelatihan, yang akan membantu membedakan class-class standar.","title":"Classification"},{"location":"#klasifikasi-data-dengan-decision-tree","text":"Ikuti langkah berikut import pandas from pandas.plotting import scatter_matrix import matplotlib.pyplot as plt get_ipython().run_line_magic('matplotlib', 'inline') import numpy as np from sklearn.tree import DecisionTreeClassifier Memanggil data yang akan di proses url = \"https://archive.ics.uci.edu/ml/machine-learning-databases/iris/iris.data\" names = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width', 'species'] iris = pandas.read_csv(url, names=names) Memisahkan data yang merupakan fitur dengan data yang merupakan label kelas. Untuk memisahkan data tersebut, maka dapat digunakan *method \u201cdrop\u201d . Jika method drop()* digunakan untuk menghapus kolom, maka sebutkan nama kolom yang ingin dihapus dan set nilai Axis=1 . Sedangkan jika baris yang ingin dihapus maka tetapkan nilai Axis=0 . Untuk data yang merupakan fitur disimpan pada variabel X , sedangkan data yang merupakan label kelas disimpan pada variabel y . X = iris.drop('species', axis=1) y = iris['species'] Membagi data menjadi data latih ( training dat a) dan data uji ( test data ). Sebelumnya perlu dilakukan import library dari sklearn. from sklearn.model_selection import train_test_split X_train, X_test, y_train, y_test = train_test_split(X, y, test_size=0.20) Membuat model klasifikasi dari data latih dan data uji yang telah ditetapkan menggunakan decision tree. classifier = DecisionTreeClassifier() classifier.fit(X_train, y_train) Menyimpan hasil prediksi yang diperoleh y_pred = classifier.predict(X_test) Menghitung akurasi dari hasil prediksi klasifikasi dengan menggunakan confussion matrix. from sklearn.metrics import classification_report, confusion_matrix print(confusion_matrix(y_test, y_pred)) print(classification_report(y_test, y_pred)) Menampilkan hasil prediksi df=pandas.DataFrame({'Actual':y_test, 'Predicted':y_pred}) df Untuk menampilkan model tree maka perlu instalasi graphvis terlebih dahulu melalui terminal. Kembali pada halaman depan jupyter notebook, pilih new, kemudian pilih terminal sehingga tampilan terminal akan muncul di tab baru. Pada terminal, ketikkan kode di bawah ini untuk menginstal graphpvis dari cloud anaconda. conda install -c conda-forge python-graphviz conda install -c conda-forge/label/broken python-graphviz conda install -c conda-forge/label/cf201901 python-graphviz Menyiapkan feature_names dan class_names dimana merupakan variabel yang perlu disiapkan untuk menggunakan graphvis . X.columns digunakan sebagai array yang menyimpan nama fitur ( feature_names ), sedangkan y.columns digunakan sebagai array untuk menyimpan nama kelas ( class_names ). X.columns = ['sepal-length', 'sepal-width', 'petal-length', 'petal-width'] y.columns = ['Iris-setosa', 'Iris-virginica', 'Iris-versicolor'] Melakukan import library graphvis terlebih dulu kemudian menyimpannya sebagai dot file. from sklearn.tree import export_graphviz # Export as dot file export_graphviz(classifier, out_file='tree.dot', feature_names = X.columns, class_names = y.columns, rounded = True, proportion = False, precision = 2, filled = True) # Convert to png using system command (requires Graphviz) from subprocess import call call(['dot', '-Tpng', 'tree.dot', '-o', 'tree.png', '-Gdpi=600']) # Display in jupyter notebook from IPython.display import Image Image(filename = 'tree.png')","title":"klasifikasi data dengan decision tree"},{"location":"#evaluasi","text":"","title":"Evaluasi"},{"location":"#metode-evaluasi-ringkasan-otomatis","text":"Ada beberapa ukuran untuk mengevaluasi ringkasan otomatis semenjak awal 2000. ROUGE adalah yang paling banyak digunakan untuk mengevaluasi secara otomatis. Berikut metode yang paling banyak digunakan ROUGE-n ROUGE-L ROUGE-W ROUGE-S ROUGE-SU","title":"Metode evaluasi Ringkasan otomatis"},{"location":"#implementasi-rouge-dengan-python","text":"Install pip install easy-rouge from rouge.rouge import rouge_n_sentence_level summary_sentence = 'the capital of China is Beijing'.split() reference_sentence = 'Beijing is the capital of China'.split() # Calculate ROUGE-2. recall, precision, rouge = rouge_n_sentence_level(summary_sentence, reference_sentence, 2) print('ROUGE-2-R', recall) print('ROUGE-2-P', precision) print('ROUGE-2-F', rouge) ROUGE-2-R 0.6 ROUGE-2-P 0.6 ROUGE-2-F 0.6 ROUGE-2-R 0.6 Implementasi ke-2 from rouge import rouge_n_sentence_level from rouge import rouge_l_sentence_level from rouge import rouge_n_summary_level from rouge import rouge_l_summary_level from rouge import rouge_w_sentence_level from rouge import rouge_w_summary_level reference_sentence = 'the police killed the gunman'.split() summary_sentence = 'the gunman police killed'.split() print('Sentence level:') score = rouge_n_sentence_level(summary_sentence, reference_sentence, 1) print('ROUGE-1: %f' % score.f1_measure) _, _, rouge_2 = rouge_n_sentence_level(summary_sentence, reference_sentence, 2) print('ROUGE-2: %f' % rouge_2) _, _, rouge_l = rouge_l_sentence_level(summary_sentence, reference_sentence) print('ROUGE-L: %f' % rouge_l) _, _, rouge_w = rouge_w_sentence_level(summary_sentence, reference_sentence) print('ROUGE-W: %f' % rouge_w) Sentence level: ROUGE-1: 0.888889 ROUGE-2: 0.571429 ROUGE-L: 0.666667 ROUGE-W: 0.550527 from rouge import rouge_n_sentence_level from rouge import rouge_l_sentence_level from rouge import rouge_n_summary_level from rouge import rouge_l_summary_level from rouge import rouge_w_sentence_level from rouge import rouge_w_summary_level reference_sentences = ['The gunman was shot dead by the police before more people got hurt'.split(), 'This tragedy causes lives of five , the gunman included'.split(), 'The motivation of the gunman remains unclear'.split(), ] summary_sentences = [ 'Police killed the gunman . no more people got hurt'.split(), 'Five people got killed including the gunman'.split(), 'It is unclear why the gunman killed people'.split(), ] print('Summary level:') _, _, rouge_1 = rouge_n_summary_level(summary_sentences, reference_sentences, 1) print('ROUGE-1: %f' % rouge_1) _, _, rouge_2 = rouge_n_summary_level(summary_sentences, reference_sentences, 2) print('ROUGE-2: %f' % rouge_2) _, _, rouge_l = rouge_l_summary_level(summary_sentences, reference_sentences) print('ROUGE-L: %f' % rouge_l) _, _, rouge_w = rouge_w_summary_level(summary_sentences, reference_sentences) print('ROUGE-W: %f' % rouge_w) Hasilnya Summary level: ROUGE-1: 0.400000 ROUGE-2: 0.188679 ROUGE-L: 0.327273 ROUGE-W: 0.200430 recall, precision, rouge_1 = rouge_n_summary_level(summary_sentences, reference_sentences, 1) print('ROUGE-2-R', recall) print('ROUGE-2-P', precision) print('ROUGE-2-F', rouge) Hasilnya ROUGE-2-R 0.36666666666666664 ROUGE-2-P 0.44 ROUGE-2-F 0.6","title":"Implementasi Rouge dengan python"}]}